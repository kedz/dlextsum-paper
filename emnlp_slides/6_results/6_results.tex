\section{Results}


\begin{frame}{Choice of Sentence Encoder: News}
  \begin{center}
  \input{6_results/6_encoder_results_news.tex}    
  \end{center}
  ~\\

  Averaging is either the \alert{\textbf{best}} encoder or 
  \textbf{statistically indistinguishable} from the best encoder!

\end{frame}

\begin{frame}{Choice of Sentence Encoder: Non-News}
  \begin{center}
    \input{6_results/6_encoder_results_nonnews.tex}    
  \end{center}
  ~\\

  Averaging is either the \alert{\textbf{best}} encoder or 
  \textbf{statistically indistinguishable} from the best encoder!
\end{frame}

\begin{frame}{Choice of Sentence Extractor: News}
    
 \begin{center}
   \input{6_results/6_extractor_results_news.tex}
 \end{center}
 
 ~\\

 \textsc{Seq2Seq} is the \alert{\textbf{best}} or \textbf{statistically 
 indistinguishable} from the best system.

 ~\\
 \textsc{Rnn} extractor as good as \textsc{SummaRunner} or 
 \textsc{Cheng \& Lapata} extractors on CNN/DailyMail data.

  

\end{frame}

\begin{frame}{Choice of Sentence Extractor: Non-News}
    
 \begin{center}
   \input{6_results/6_extractor_results_nonnews.tex}
 \end{center}

 ~\\

 \textsc{Seq2Seq} is the \alert{\textbf{best}} or statistically indistinguishable from the best
 system.


\end{frame}



\begin{frame}{Word Embedding Fine-Tuning: (News)}
  \begin{center}
    \begin{tabular}{ccccc}
 & & \multicolumn{3}{c}{\textbf{Rouge-2 Recall}}\\
      \toprule
        \textbf{Ext} & \textbf{Emb}  & 
           \textbf{CNN/DM} & 
           \textbf{NYT} & 
           \textbf{DUC} \\
      \midrule
      \multirow{2}{*}{\textsc{Seq2Seq}} 
        & Fixed & \textbf{25.6} & \textbf{35.7} & \textbf{22.8} \\
        & Fine-Tuned &         25.3  & \textbf{35.7} & \textbf{22.9} \\
      \hline
      \multirow{2}{*}{\textsc{Cheng \& Lapata}} 
        & Fixed & \textbf{25.3} & \textbf{35.6} & \textbf{23.1} \\
        & Fine-Tuned &         24.9  &         35.4  & \textbf{23.0} \\
      \bottomrule
  \end{tabular}
 \end{center}

 ~\\
 
 Performance difference when using \textit{fixed} embeddings versus 
 \textit{fine-tuned} embeddings.
 
  ~\\

  Models are using the \textsc{Avg} encoder and are initialized with 
  Glove embeddings. 

  ~\\

  \textbf{No statistically significant improvement} on news with fine-tuning!

\end{frame}

\begin{frame}{Word Embedding Fine-Tuning: Non-News}
 \begin{center}
  \begin{tabular}{ccccc}
   \toprule
   \textbf{Ext} & \textbf{Emb}  & 
        \textbf{Reddit} & \textbf{AMI} & \textbf{PubMed} \\
   \midrule
   \multirow{2}{*}{\textsc{Seq2Seq}}
      & Fixed & \textbf{13.6} &         5.5  & \textbf{17.7} \\
      & Fine-Tuned & \textbf{13.8} & \textbf{5.8} &         16.9  \\
   \hline
   \multirow{2}{*}{\textsc{Cheng \& Lapata}} 
      & Fixed & \textbf{13.6} & \textbf{6.1} & \textbf{17.7} \\
      & Fine-Tuned & \textbf{13.4} & \textbf{6.2} & \textbf{16.4} \\
   \bottomrule
  \end{tabular}
 \end{center}

 ~\\
 
 Performance difference when using \textit{fixed} embeddings versus 
 \textit{fine-tuned} embeddings.
 
 ~\\

 All models are using the \textsc{Avg} encoder and are initialized with 
 pretrained Glove embeddings from gigaword/wikipedia. 

 ~\\
 \textbf{Statistically significant improvement} with \textsc{Seq2Seq} on AMI
 data.\\
 Otherwise, same trend as news, no improvement using fine-tuning.

\end{frame}

\begin{frame}{Word Class Ablation: ROUGE-2 Recall}
 \begin{center}
  \begin{tabular}{lcccccc}
   \toprule
   \multirow{1}{*}{\textbf{Ablation}} & 
            \textbf{CNN/DM} & \textbf{NYT} & \textbf{DUC} &
            \textbf{Reddit} & \textbf{AMI} & \textbf{PubMed} \\
   \midrule
     All Words & $\mathbf{25.4}$ & $\mathbf{34.7}$ & $22.7$ &
                  $\mathbf{11.4}$ & $5.5$ & $\mathbf{17.0}$  \\
     $-$ Nouns & $25.3^\dagger$  & $34.3^\dagger $ & $22.3^\dagger$  &
   $10.3^\dagger$ & \alert<3>{$3.8^\dagger$} & \alert<3>{$15.7^\dagger$} \\
     $-$ Verbs & $25.3^\dagger$  & $34.4^\dagger $ & $22.4^\dagger$ &
            $10.8$ & $5.8$ & $16.6^\dagger$ \\
 $-$ Adj/Adv & 
  $25.3^\dagger$ & $34.4^\dagger$ & $22.5$ &
   \alert<4>{$9.5^\dagger$} & $5.4$ & $16.8^\dagger$ \\
   $-$ Function & $25.2^\dagger$ & $34.6^\dagger$ & \alert<2>{$\mathbf{22.9}^\dagger$} &
   $10.3^\dagger$ & \alert<2>{$\mathbf{6.3}^\dagger$} & $16.6^\dagger$ \\
   \bottomrule
  \end{tabular}
 \end{center}

 ~\\

 RNN extractor with Averaging encoder. 

 ~\\

 \textbf{Bold} is best performance. $\dagger$ indicates significant 
 difference from All Words, i.e. non-ablated model.

 ~\\



\end{frame}



\begin{frame}{Shuffled vs In-Order (News)}

 \begin{center}
  \begin{tabular}{ccL{2cm}m{1cm}L{.75cm}} 
   \toprule
   \textbf{Ext.} & \textbf{Order} & 
                           \textbf{CNN/DM} & \textbf{NYT} & \textbf{DUC} \\
   \midrule
%?   \multirow{2}{*}{RNN} 
%?       & In-Order & 
%?       \textbf{25.4} & \textbf{34.7} & \textbf{22.7} \\
%?       & Shuffled & 
%?               22.8 &          25.0  &         18.2  \\
   \hline
   \multirow{2}{*}{Seq2Seq}
       & In-Order & 
       \textbf{25.6} & \textbf{35.7} &  \textbf{22.8} \\
       & Shuffled & 
               21.7  &         25.6  &          21.2  \\
   \bottomrule
  \end{tabular}
 \end{center}

 ~\\

 Shuffled model is trained on shuffled sentence order documents.

 ~\\

 Both models evaluated on in-order data.

\end{frame}

\begin{frame}{Shuffled vs In-Order (Other)}

 \begin{center}
  \begin{tabular}{ccccc} 
   \toprule
   \textbf{Ext.} & \textbf{Order} & 
                           \textbf{Reddit} & \textbf{AMI} & \textbf{PubMed} \\
   \midrule
   \multirow{2}{*}{Seq2Seq}
       & In-Order & 
       \textbf{13.6} &         5.5  &  \textbf{17.7} \\
       & Shuffled & 
       \textbf{13.5} & \textbf{6.0} &          14.9  \\
   \bottomrule
  \end{tabular}
 \end{center}

 ~\\

 Shuffled model is trained on shuffled sentence order documents.

 ~\\

 Both models evaluated on in-order data.

\end{frame}

