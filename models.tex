\hal{i'm having a hard time understanding what's new and what's prior work here.}


At a high level, all the models considered in this paper share the same two part structure: 
\textit{i) a sentence encoder} which each sentence $\sent_i$ (treated as a sequence of tokens) to 
an embedding $\sentvec_i \in \mathcal{R}^d$, \hal{notation class, you used $d$ already for number of sentences} and 
\textit{ii) a sentence extractor} which takes as input all of a document's 
sentence embeddings and predicts which sentences to extract to produce the 
extract summary. The sentence extractor is essentially a discriminative 
classifier $p(\slabel_1, \ldots, \slabel_{\docsize}| \sentvec_1, \ldots, \sentvec_{\docsize})$.

Depending on the architectural choices of each component we propose we 
can recover the specific implementations of \cite{cheng&lapata} and 
\cite{nallapati}, which we outline below.

\subsection{Sentence Encoders}
\input{sentence_encoders.tex}

\subsection{Sentence Extractors}
\input{sentence_extractors.tex}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "dlextsum.emnlp18"
%%% End:
