%\hal{i'm having a hard time understanding what's new and what's prior work here.}

\kathy{This section feels very long. Do you think all the formulas are needed? They take up a lot of space. Would be good for Hal and Chris H to see this comment.}
\hal{agree. this section is about 3 pages long. i would suggest moving low-level details to an appendix which can be submitted as supplemental. i've added toappendix.sty to make this easy. i've given an example usage in the RNN encoder section which i think you can use throughout. i also added a makefile that will split the paper into main and supllemental}

For a typical deep learning model of extractive 
summarization there are two main design decisions:
%At a high level, all the models considered in this paper share the same two part structure: 
\textit{i)}  the choice of \textit{sentence encoder} 
which maps each sentence \sent[i] 
%(treated as a sequence of word embeddings) 
to an embedding $\sentEmb[i] \in \mathcal{R}^{\sentEmbSize}$, 
%\hal{notation class, you used $d$ already for number of sentences} 
and 
\textit{ii)} the choice of \textit{sentence extractor} 
which maps a sequence of sentence embeddings 
$\sentEmb = \sentEmb[1],\ldots, \sentEmb[\docSize]$  
to a sequence of extraction
decisions $\slabel = \slabel_1,\ldots,\slabel_{\docSize}$.
The sentence extractor is then a discriminative 
classifier $p(\slabel | \sentEmb)$.
%and predicts which sentences to extract to produce the 
%extract summary. 

We study three architectures for the sentence encoders, namely, 
embedding averaging, RNNs, and 
CNNs.
We also propose two simple models for the sentence extractor and compare
to the previously proposed extractors of 
\citet{cheng2016neural} and \citet{nallapati2017summarunner}.
\hal{i think it's still confusing what's new and what's not. maybe you can somewhat mark? like things with $\star$ are new and ones without are old or something?}
The prior works differ significantly but make the same semi-Markovian
factorization of the extraction decisions, i.e. 
$p(\slabel|\sentEmb)=\prod_{i=1}^\docSize p(\slabel[i]|\slabel[<i],\sentEmb)$,
where each prediction \slabel[i] is dependent on all previous \slabel[j] for
all $j < i$.
By contrast, our extractors make a stronger conditional independence 
assumption $p(\slabel|\sentEmb)=\prod_{i=1}^\docSize p(\slabel[i]|\sentEmb)$,
essentially making independent predictions conditioned on $\sentEmb$.
In theory, our models should perform worse because of this, however, as
we later show, this is not the case empirically.



\hal{i think you might need a subsection at the end of this section with oen or two paragraphs of compare/contrast the different models, esp if details are going to appendix}


%Depending on the architectural choices of each component we propose we 
%can recover the specific implementations of \cite{cheng&lapata} and 
%\cite{nallapati}, which we outline below.

\subsection{Sentence Encoders}
\input{sentence_encoders}

\subsection{Sentence Extractors}
\input{sentence_extractors}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "dlextsum.emnlp18"
%%% End:
