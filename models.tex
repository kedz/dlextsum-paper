At a high level, all of our models share the same two part structure: 
\textit{i) a sentence encoder} which maps an arbitrary sequence of tokens to 
an embedding $\sentvec \in \mathcal{R}^d$, and 
\textit{ii) a sentence extractor} which takes as input all of a document's 
sentence embeddings and predicts which sentences to extract to produce the 
extract summary. The sentence extractor is essentially a discriminative 
classifier $p(\slabel_1, \ldots, \slabel_{\docsize}| \sentvec_1, \ldots, \sentvec_{\docsize})$.

Depending on the architectural choices of each component we propose we 
can recover the specific implementations of \cite{cheng&lapata} and 
\cite{nallapati}, which we outline below.

\subsection{Sentence Encoders}
\input{sentence_encoders.tex}

\subsection{Sentence Extractors}
\input{sentence_extractors.tex}
