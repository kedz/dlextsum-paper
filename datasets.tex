We perform our experiments across six corpora from varying domains to 
understand how different biases within each domain can affect content 
selection. The corpora come from the news domain
(CNN-DailyMail, New York Times, DUC), personal narratives domain (Reddit),
workplace meetings (AMI), and medical journal articles (PubMed). See 
\autoref{tab:data} for dataset statistics.


\paragraph{CNN-DailyMail} We use the preprocessing and training, validation, 
and test splits
of \cite{see2017get}.
This corpus is a mix of news on different topics including politics,
sports, and entertainment.

\paragraph{New York Times}The New York Times (NYT) corpus \cite{sandhaus2008new} contains
 two types of abstracts for a subset of its articles. The first summary is
an abstract \textcolor{red}{produced by an archival librarian} and the 
second is an online teaser meant to elicit a viewer on the webpage to
click to read more. From this collection we take all articles that have 
a combined summary length of at least 100 words
%\hal{i'm not sure what you mean here. are you concatenating the two summaries? if so, say so. is that a little weird tho?}. This collection
%includes both straight newswire as well as opinion and long-form journalism.
We create training, validation, and test splits by partitioning on dates;
we use the year 2005 as the validation data, with training and test partitions
including documents before and after 2005 respectively.
%yielding $44,382$ training, $5,523$ validation, and $6,495$ test documents.

\paragraph{DUC} We use the single document summarization data from the 2001
and 2002
Document Understanding Conferences (DUC) \cite{over2002introduction}. We split the 2001 data into training
and validation splits and reserve the 2002 data for testing.
%, resulting in
%516/91/657 documents for training, validation, and test respectively. 
%The test set has two or three human abstracts roughly 100 words in length per 
%articles.}

\paragraph{AMI} The AMI corpus \cite{carletta2005ami} 
is collection of real and staged office meetings
annotated with text transcriptions, along with abstractive
summaries. We use the prescribed splits. 
%to get 98/19/20 training, validation,
%and test examples with one human abstract summary per meeting. 
%We ignore any speaker information since we are primarily
%interested in studying content selection in a domain agnostic way.
%The summaries are about 290 words long on average and so we target this length
%for summary generation. \hal{a) you don't need braces around paragraph text. b) this is the only place you mention task stuff rather than dataset stuff. mabye move target length information to the experiments section?}


\paragraph{Reddit} \citet{ouyang2017crowd} collected a corpus of personal 
    stories shared
 on Reddit\footnote{\url{www.reddit.com}} along with multiple extractive 
 and abstractive summaries. %These stories are relatively short compared
 %to the other corpora with an average sentence length of ??. 
%\kathy{Why didn't you use theirs?}
% We created our own train, validation, and test splits resulting in 
%404/24/48 documents respectively. 
%}

\paragraph{PubMed}{We created a corpus of 25,000 randomly samples 
    medical journal articles from the PubMed Open Access 
    Subset\footnote{\url{https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/}}.
    We only included articles if they were at least 1000 words long and 
    had an abstract of at least 50 words in length.
We used the article abstracts as the ground truth human summaries.}
\hal{maybe mention this in the intro with a footnote about data release? we've recently used statements like the following in a footnote:
  \textbf{Code and data release:} Upon publication, all code and pre-processing scripts will be released under an MIT (or more liberal) license; all data will be made available after publication when allowed by original licenses.}

\subsection{Ground Truth Extract Summaries}
Since we do not typically have ground truth extract summaries from which to
create the labels $\slabel_i$, we construct gold label sequences 
by greedily optimizing ROUGE-1, as follows.
%?\begin{algorithm}[H]
%?    \KwData{$I = \{\sent[1], \ldots, \sent[n]\}$, word budget $c$} 
%?    empty summary $\summary = \emptyset$\\
%?    $y_i = 0 \quad \forall i \in 1, \ldots, n$ \\
%?    \While{$\sum_{\sent[i] \in \summary} |\sent[i]| \le c \;$}{
%?        $\hat{\sent} = {\argmax}_{ \sent[i] \in I \setminus \summary} \rouge(\summary \cup \sent[i])$\\
%?  \eIf{$\rouge(\summary \cup \hat{\sent}) > \rouge(\summary)$}{
%?       $\summary \gets \summary \cup \hat{\sent}$
%?   }{\textbf{break}\\}
%?                                       }
%?$y_i \gets 1 $ if $s_i \in S \; \forall i \in 1, \ldots, n$\\ 
%?\KwResult{$y = y_1,\ldots y_n$}
%?%                                        \caption{How to write algorithms}
%?\end{algorithm}


\begin{algorithmic}

\State $I = \{\sent[1], \ldots, \sent[n]\};\quad y_i = 0 \quad \forall i \in 1, \ldots, n$
\State    empty summary $\summary = \emptyset$; ~~word budget $c$
\While{$\sum_{\sent[i] \in \summary} |\sent[i]| \le c \;$} 
\State $\hat{i} = {\argmax}_{ i \in \{j \in [n]| \sent[j] \notin \summary\}} \rouge(\summary \cup \sent[i])$
\If {$\rouge(\summary \cup \sent[\hat{i}]) > \rouge(\summary)$}
\State $\summary \gets \summary \cup \sent[\hat{i}]; \quad y_{\hat{i}} = 1$
        \Else
        \State \textbf{break} 
       \EndIf
\EndWhile
\State \Return $y=y_1, \ldots, y_n$
\end{algorithmic}

%\begin{enumerate}
%\item Start with an empty summary $\summary = 
%\null \emptyset$ and word limit $c$. 
%\item while $\sum_{i \in \summary} |\sent[i]| \le c$:
%    \begin{enumerate}
%        \item $\hat{\sent} = {\argmax}_{ i \in [\docSize] \setminus \summary} \operatorname{ROUGE}(\summary \cup \sent[i])$
%\item if $\operatorname{ROUGE}(\summary \cup \hat{\sent}) > \operatorname{ROUGE}(\summary)$
%    \begin{enumerate}
%        \item $\summary \leftarrow \summary \cup \hat{\sent}$
%    \end{enumerate}
%\item else break
%
%%to $\summary$ 
%\end{enumerate}
%\item return $y_i =\begin{cases} 1 & \textrm{if $i \in S$}\\ 0 & \textrm{otherwise} \end{cases}$ for all $i \in [n]$.
%\end{enumerate}
%stopping when the ROUGE-1 score no longer increases or the 
%length budget is reached\hal{what does this mean. does it mean that if the $\hat s$ that's selected puts you over length limit then you stop, or is it that you stop when there are no more sentences that you can cram in? and what if adding $\hat s$ doesn't make rouge go up?}. 
We choose to optimize for ROUGE-1 rather than 
ROUGE-2 similarly to other optimization based approaches to summarization 
\textcolor{red}{
\cite{durrett2016learning,sipos2012large,nallapati2017summarunner} which found this
be the easier optimization target.}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "dlextsum.emnlp18"
%%% End:
