We perform our experiments across six corpora from varying domains to 
understand how different biases within each domain can affect content 
selection. The corpora come from the news domain
(CNN-DailyMail, New York Times, DUC), personal narratives domain (Reddit),
workplace meetings (AMI), and medical journal articles (PubMed). See 
\autoref{tab:data} for dataset statistics.


\paragraph{CNN-DailyMail} We use the preprocessing and training, validation, 
and test splits
of \citet{see2017get}.
This corpus is a mix of news on different topics including politics,
sports, and entertainment.

\paragraph{New York Times}The New York Times (NYT) corpus \cite{sandhaus2008new} contains
 two types of abstracts for a subset of its articles. The first summary is
an archival abstract and the 
second is a shorter online teaser meant to entice a viewer of the webpage to
click to read more. From this collection, we take all articles that have 
a concatenated summary length of at least 100 words.
We create training, validation, and test splits by partitioning on dates;
we use the year 2005 as the validation data, with training and test partitions
including documents before and after 2005 respectively.

\paragraph{DUC} We use the single document summarization data from the 2001
and 2002
Document Understanding Conferences (DUC) \cite{over2002introduction}. We split the 2001 data into training
and validation splits and reserve the 2002 data for testing.

\paragraph{AMI} The AMI corpus \cite{carletta2005ami} 
is a collection of real and staged office meetings
annotated with text transcriptions, along with abstractive
summaries. We use the prescribed splits. 

\paragraph{Reddit} \citet{ouyang2017crowd} collected a corpus of personal 
    stories shared
 on Reddit\footnote{\url{www.reddit.com}} along with multiple extractive 
 and abstractive summaries. We randomly split this data using roughly three and five percent of the data validation and test respectively.

\paragraph{PubMed}{We created a corpus of 25,000 randomly samples 
    medical journal articles from the PubMed Open Access 
    Subset\footnote{\url{https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/}}.
    We only included articles if they were at least 1000 words long and 
    had an abstract of at least 50 words in length.
We used the article abstracts as the ground truth human summaries.}

\subsection{Ground Truth Extract Summaries}
Since we do not typically have ground truth extract summaries from which to
create the labels $\slabel_i$, we construct gold label sequences 
by greedily optimizing ROUGE-1, using the algorithm in \autoref{app:oracle}.
We choose to optimize for ROUGE-1 rather than 
ROUGE-2 similarly to other optimization based approaches to summarization 
\cite{sipos2012large,durrett2016learning} which found this to
be the easier target to learn.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "dlextsum.emnlp18"
%%% End:
