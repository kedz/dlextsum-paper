
\begin{table*}[t]
\center
%\begin{tabular}{llgcgcg}
%    \toprule
%    \multirow{2}{*}{\textbf{Extractor}} &\multirow{2}{*}{\textbf{Embeddings}}  & \multicolumn{1}{g}{\textbf{CNN/DM}} & \multicolumn{1}{c}{\textbf{NYT}} & \multicolumn{1}{g}{\textbf{DUC 2002}} & \multicolumn{1}{c}{\textbf{Reddit}} & \multicolumn{1}{g}{\textbf{AMI}}\\
%     &  & R-2 & R-2 & R-2 & R-2 & R-2\\
%    \hline
%    \multirow{2}{*}{RNN} & Fixed & \textbf{25.42} & \textbf{34.67} & \textbf{22.65} & \textbf{11.37} & \textbf{5.50}\\
%     & Learned & 25.21 & 34.31 & \textbf{22.60} & \textbf{11.30} & \textbf{5.30}\\
%    \hline
%    \multirow{2}{*}{Seq2Seq} & Fixed & \textbf{25.56} & \textbf{35.73} & \textbf{22.84} & \textbf{13.61} & 5.52\\
%     & Learned & 25.34 & \textbf{35.65} & \textbf{22.88} & \textbf{13.78} & \textbf{5.79}\\
%    \hline
%    \multirow{2}{*}{C\&L} & Fixed & \textbf{25.29} & \textbf{35.58} & \textbf{23.11} & \textbf{13.65} & \textbf{6.12}\\
%     & Learned & 24.95 & 35.41 & \textbf{22.98} & \textbf{13.39} & \textbf{6.16}\\
%    \hline
%    \multirow{2}{*}{SummaRunner} & Fixed & \textbf{25.44} & \textbf{35.40} & \textbf{22.28} & \textbf{13.41} & \textbf{5.63}\\
%     & Learned & 25.11 & 35.20 & \textbf{22.15} & 12.64 & \textbf{5.85}\\
%    \bottomrule
%\end{tabular}
\begin{tabular}{ccgcgcgc}
    \toprule
    \multirow{2}{*}{\textbf{Extractor}} &\multirow{2}{*}{\textbf{Embeddings}}  & \multicolumn{1}{g}{\textbf{CNN/DM}} & \multicolumn{1}{c}{\textbf{NYT}} & \multicolumn{1}{g}{\textbf{DUC 2002}} & \multicolumn{1}{c}{\textbf{Reddit}} & \multicolumn{1}{g}{\textbf{AMI}} & \multicolumn{1}{c}{\textbf{PubMed}}\\
     &  & R-2 & R-2 & R-2 & R-2 & R-2 & R-2\\
    \hline
    \multirow{2}{*}{RNN} & Fixed & \textbf{25.42} & \textbf{34.66} & \textbf{22.65} & \textbf{11.37} & \textbf{5.50} & \textbf{17.03}\\
     & Learned & 25.21 & 34.31 & \textbf{22.60} & \textbf{11.30} & \textbf{5.30} & 16.44\\
    \hline
    \multirow{2}{*}{Seq2Seq} & Fixed & \textbf{25.56} & \textbf{35.73} & \textbf{22.84} & \textbf{13.61} & 5.52 & \textbf{17.68}\\
     & Learned & 25.34 & \textbf{35.65} & \textbf{22.88} & \textbf{13.78} & \textbf{5.79} & 16.94\\
    \hline
    \multirow{2}{*}{C\&L} & Fixed & \textbf{25.28} & \textbf{35.58} & \textbf{23.11} & \textbf{13.65} & \textbf{6.12} & \textbf{17.65}\\
     & Learned & 24.95 & 35.41 & \textbf{22.98} & \textbf{13.39} & \textbf{6.16} & 16.40\\
    \hline
    \multirow{2}{*}{SummaRunner} & Fixed & \textbf{25.44} & \textbf{35.40} & \textbf{22.28} & \textbf{13.41} & \textbf{5.63} & \textbf{17.23}\\
     & Learned & 25.11 & 35.20 & \textbf{22.15} & 12.64 & \textbf{5.85} & 16.85\\
    \bottomrule
\end{tabular}


\caption{ROUGE-2 recall across sentence extractors
    when using learned or pretrained embeddings. In both cases embeddings
    are initialized with pretrained GloVe embeddings. All results are 
averaged from five random initializations. All extractors use the averaging 
sentence encoder. When both learned and unlearned settings are bolded,
there is no signifcant performance difference.}
\label{tab:embeddings}
\end{table*}

%\begin{table*}
%\center
%\begin{tabular}{| c | c || c | c | c | c | c | c | c | c |}
%\hline
%  &   & \multicolumn{2}{|c|}{cnn-dailymail} & \multicolumn{2}{|c|}{nyt} & \multicolumn{2}{|c|}{duc-sds} & \multicolumn{2}{|c|}{reddit} \\
%system & embeddings & R1 & R2  & R1 & R2  & R1 & R2  & R1 & R2  \\
%\hline
%\multirow{2}{*}{RNN} & fixed & 55.3 & 25.4 & 51.4 & 34.7 & 44.1 & 22.6 & 45.2 & 11.4\\ \cline{2-10}
% & learned & 55.1 & 25.2 & 51.1 & 34.3 & 44.1 & 22.6 & 45.3 & 11.3\\
%\hline
%\multirow{2}{*}{Seq2Seq} & fixed & 55.6 & 25.6 & 52.5 & 35.7 & 44.4 & 22.8 & 49.1 & 13.6\\ \cline{2-10}
% & learned & 55.2 & 25.3 & 52.4 & 35.7 & 44.5 & 22.9 & 49.4 & 13.8\\
%\hline
%\multirow{2}{*}{C\&L} & fixed & 55.1 & 25.3 & 52.3 & 35.6 & 44.8 & 23.1 & 48.3 & 13.6\\ \cline{2-10}
% & learned & 54.8 & 25.0 & 52.1 & 35.4 & 44.6 & 23.0 & 48.6 & 13.5\\
%\hline
%\multirow{2}{*}{SummaRunner} & fixed & 55.3 & 25.4 & 52.1 & 35.4 & 44.0 & 22.3 & 48.8 & 13.4\\ \cline{2-10}
% & learned & 55.0 & 25.1 & 52.0 & 35.2 & 43.8 & 22.1 & 47.8 & 12.6\\
%\hline
%\end{tabular}
%\caption{ROUGE 1 and 2 recall results across different sentence extractors
%    when using learned or pretrained embeddings. In both cases embeddings
%    are initialized with pretrained GloVe embeddings. All results are 
%averaged from five random initializations. All extractors use the averaging 
%sentence encoder.}
%\label{tab:embeddings}
%\end{table*}
