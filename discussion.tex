Learning in the news domain is severely inhibited by the lead bias. 
The output of all systems is highly similar to the lead baseline,
with \textcolor{red}{??\% of all system output sentences also occurring
in the lead summary.} Shuffling improves this 
\textcolor{red}{(reducing to ??\%)} but the overall system performance drops
    significantly.
    The relative robustness of the news domain to POS ablation also suggests
    suggests that models are mostly learning to recognize the stylistic 
    features unique to the beggining of the article, and not the content.
    Additionally, drop in performance when learning word embeddings on 
    the news domain suggests that word embeddings alone do not provide 
    very generalizable content features compared to recognizing the lead.

The picture is rosier for non-news summarization where POS ablation leads
to larger performance differences and shuffling either does not inhibit content
selection significantly or leads to modest gains. In order to learn better
word-level representations on these domains will likely require much
larger corpora, somthing which might remain unlikely for personal narratives
and meetings.



The lack of distinction amongst sentence encoders is interesting because 
it echoes findings in the generic sentence embedding literature 
where word embedding averaging is frustratingly difficult to 
outperform  \cite{wieting2015towards,arora2016simple,wieting2017revisiting}.
The inability to learn useful sentence representations is also 
borne out in the 
SummaRunner model, where there are explicit similarity computations
between document or summary representations and sentence embeddings;
these computations do not seem to add much to the performance as the 
Cheng \& Lapata and Seq2Seq models which lack these features generally
perform as well or better.
Furthermore, the Cheng \& Lapata and SummaRunner extractors both construct
a history of previous selection decisions to inform future choices but this
does not seem to significantly improve performance over the Seq2Seq extractor 
which does not, suggesting that we need to rethink or find novel forms 
of sentence representation for the summarization task.


A manual examination of the outputs revealed some interesting failure modes
although in general it was hard to discern clear patterns of behaviour 
other than lead bias. On the news domain, the models consistently learned 
to ignore quoted material in the lead, as often the quotes provide
color to the story but are unlikely to be included in the summary (e.g.\textit{``It was like somebody slugging a punching bag.''}). 
This behavior was most likely triggered by the presence of quotes, as the
quote attributions, which were often tokenized as separate sentences,
would subsequently be included in the summary despite also not containing 
much information 
(e.g. \textit{Gil Clark of the National Hurricane Center said Thursday}).




The Reddit corpus was particularly challenging as often there was no concise
way to extractively represent the story. Authors often inserted a 
fairly brief summary of the story, either at the end or beginning, but
this was so abstractive as to not have much overlap with the reference
summaries. For example, the first sentence, \textit{I took a dog off the street, and she changed my grandparent's lives}, has little overlap with the reference
\textit{The dog I found for my grandparents still gets really excited to see
me whenever I come to visit}, but is still functionally a reasonable 
summary of the story. These ``micro summaries'' do not seem to be 
consistently found by any summarization model. 



