\begin{figure*}
  \center
  \includegraphics[scale=.7]{figures/rnnextractor.pdf}
  \includegraphics[scale=.7]{figures/s2s_extractor.pdf}
  \includegraphics[scale=.7]{figures/clextractor.pdf}
  \includegraphics[scale=.7]{figures/rnnextractor.pdf}
  \caption{Sentence extractor architectures: a) \modelOneBF, b) \modelTwoBF,
            c) \baselineOneBF, and d) \baselineTwoBF. }
  \label{fig:extractors}
\end{figure*}


Given a sequence of sentence embeddings $\sentvec_i = \senc(\sent_i)$,
a sentence extractor produces a conditional distribution over the 
corresponding sentence extraction variables 
$p(\slabel_1,\ldots,\slabel_{\sentSize}|\sentvec_1, \ldots, \sentvec_{\sentSize})$.
We propose two simple recurrent neural network based sentence extractors
that make a strong conditional independence assumption over the labels
$\slabel_i$,chiefly 
$\explicitLikelihood= \naiveLikelihood$. This stands in contrast to our 
baseline models which make a weaker assumption, 
$\compactLikelihood = \markovLikelihood$, at the expense of greater 
computational complexity. See Figure~\ref{fig:extractors} for a diagram of the 
four sentence extractor architectures.

\textbf{RNN Extractor} Our first proposed model is a very simple bidirectional
RNN based tagging model. As in the RNN sentence encoder we use a GRU cell.
The the forward and backward outputs of each sentence are passed through a 
single layer perceptron with a logsitic sigmoid output (denoted by $\sigma$)
to predict the probability
of extracting each sentence:
\begin{align}
    \rExtHidden_i &= \rgru(\sentvec_i, \rExtHidden_{i-1}) \\
   \lExtHidden_i &= \lgru(\sentvec_i, \lExtHidden_{i+1}) \\
   \logits_i &= \relu\left(U \cdot [\rExtHidden_i; \lExtHidden_i] + u \right)\\
   p(\slabel_i=1|\sentvec) &= \sigma\left(V\cdot \logits_i + v  \right)
\end{align}
for all $i \in 1, \ldots, \docsize$. $\rgru$ amd $\lgru$ indicate the 
forward and backward GRUs respectively, and each have separate learned 
parameters; $U, V$ and $u, v$ are learned weight and bias parameters.
\textcolor{red}{The initial states $\rExtHidden_0$ and $\lExtHidden_{\docsize + 1}$ are not 
learned and are set to zero vectors.}



\textbf{\sts~Extractor} One shortcoming of the RNN extractor is that long range
information from one end of the document may not easily be able to effect 
extraction probabilities of sentences at the other end. 
Our second proposed model, which we refer to as the \stsbf~extractor,
is based on the attentional sequence-to-sequence models commonly
used for neural machine translation \cite{badhanau} and 
abstractive summarization \cite{see}. The sentence embeddings are first
encoded by a bidirectional $\gru$. A separate decoder $\gru$ transforms each 
sentence into a query vector which attends to the encoder output. The
attention weighted encoder output and the decoder $\gru$ output are concatenated
and fed into a multi-layer percepron to compute the extraction probability.
Formally we have:
\begin{align}
  \rEncExtHidden_i &= \rgru_{enc}(\sentvec_i, \rEncExtHidden_{i-1}) \\
  \lEncExtHidden_i &= \lgru_{enc}(\sentvec_i, \lEncExtHidden_{i+1}) 
  \rDecExtHidden_i &= \rgru_{dec}(\sentvec_i, \rDecExtHidden_{i-1}) \\
  \lDecExtHidden_i &= \lgru_{dec}(\sentvec_i, \lDecExtHidden_{i+1}) 
\end{align}
\begin{align}
 \decExtHidden_i = [\rDecExtHidden_i; \lDecExtHidden_i], &\;\;
 \encExtHidden_i = [\rEncExtHidden_i; \lEncExtHidden_i] 
\end{align}
\begin{align}
 \alpha_{i,j} = 
   \frac{\exp \left(\decExtHidden_i \cdot \encExtHidden_j \right)}{
   \sum_{j=1}^{\docsize}\exp\left(\decExtHidden_i \cdot \encExtHidden_j\right)}, 
& \;\; \attnExtHidden_i = \sum_{j=1}^{\docsize} \alpha_{i,j} \encExtHidden_j 
\end{align}
\begin{align}
   \logits_i = \relu\left(U \cdot [\attnExtHidden_i; \decExtHidden_i] + u \right)&\\
   p(\slabel_i=1|\sentvec) = \sigma\left(V\cdot \logits_i + v  \right),
\end{align}
for all $i \in 1, \ldots, \docsize$.
The final outputs of each encoder direction are passed to first decoder
steps; additionally, the first step of the decoder GRUs are learened 
``begin decoding'' vectors $\rDecExtHidden_0$ and $\lDecExtHidden_0$ (
see Figure~\ref{fig:extractors}.b).
Each GRU has separate learned 
parameters; $U, V$ and $u, v$ are learned weight and bias parameters.
\textcolor{red}{The initial states $\rExtHidden_0$ and $\lExtHidden_{\docsize + 1}$ are not 
learned and are set to zero vectors.}


\textbf{Cheng \& Lapata Extractor} 
We compare the previously proposed architectures to the sentence extractor
model of \cite{chenglapata}. Unlike the previous models where
sentence extraction predictions are conditionally independent given
the sentence embeddings, this model uses previous extraction probabilities to
influence later decisions. The basic architecture is a unidirectional
sequence-to-seqeunce
model defined as follows:
\begin{align}
    \encExtHidden_i &= \gru_{enc}(\sentvec_i, \encExtHidden_{i-1}) \\
    \decExtHidden_i &= \gru_{dec}(p_{i-1} \cdot \sentvec_{i-1}, \decExtHidden_{i-1}) \\
   \logits_i &= \relu\left(U \cdot [\encExtHidden_i; \decExtHidden_i] + u \right)\\
    p_i &= p(\slabel_i=1|\slabel_{<i}, \sentvec) = \sigma\left(V\cdot \logits_i + v  \right) 
\end{align}
for all $i \in 1, \ldots, \docsize$.
The final output of the encoder is passed to the first decoder
step; additionally, the first step of the decoder GRU is a learned 
``begin decoding'' vector $\decExtHidden_0$ (
see Figure~\ref{fig:extractors}.c).
Each GRU has separate learned 
parameters; $U, V$ and $u, v$ are learned weight and bias parameters.
\textcolor{red}{The initial states $\rExtHidden_0$ and $\lExtHidden_{\docsize + 1}$ are not 
learned and are set to zero vectors.}
 Note in \textcolor{red}{Equation 18} that 
the decoder side GRU input is the sentence embedding from the previous time
step weighted by its probabilitiy of extraction ($p_{i-1}$) from the 
previous step.


\textbf{SummaRunner Extractor}
\begin{align}
    \rEncExtHidden_i &= \rgru(\sentvec_i, \rEncExtHidden_{i-1}) \\
    \lEncExtHidden_i &= \lgru(\sentvec_i, \lEncExtHidden_{i+1}) \\
    q &= \tanh\left(b_q + W_q\frac{1}{\docsize}\sum_{i=1}^{\docsize} [\rEncExtHidden_i; \lEncExtHidden_i] \right)\\
    \extHidden_i &= \relu\left(b_z + W_z [\rEncExtHidden_i; \lEncExtHidden_i]\right)
  \end{align}
  \begin{align*}
      p(y_i=1|y_{<i}, h) = \sigma(& W_{con}\cdot z_i \\
                     & + z_i^T W_{sal}\cdot q \\
                     & -z_i^T W_{nov} \cdot \tanh(g_i) \\
                     & + b_{rp_i}  \\
                     & + b_{ap_i} \\
                     & + b)     \\
      g_j & = \sum_{i=1}^{j-1} p(y_j=1|y_{<j},h) \cdot z_j
\end{align*}


