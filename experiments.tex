
We evaluate summary quality using ROUGE-2 recall \cite{lin2004rouge};
ROUGE-1 and ROUGE-LCS trend similarity in our experiments.
We use target word lengths of 100 words for news, and 
75, 290, and 200 for Reddit, AMI, and PubMed respectively.
We also evaluate using METEOR \cite{denkowski:lavie:meteor-wmt:2014}.\footnote{We use the default settings for METEOR and use remove stopwords and no stemming options for ROUGE, keeping defaults for all other parameters.}
Summaries are generated by extracting the top ranked sentences by model probability $p(y_i=1|y_{<i},h)$, stopping when the word budget is met or exceeded.
%which measures precision and recall of reference words, allowing for
%more matchings on synonymy or morphology.
We estimate statistical significance by averaging each document level score
over the five random initializations. 
We then test the difference between the best system on each dataset and 
all other systems using the approximate randomization test 
\cite{riezler2005some} with the Bonferroni correction for multiple comparisons,
testing for significance at the $0.05$ level. 

\subsection{Training}

We train all models to minimize the weighted negative log-likelihood
\[\mathcal{L} = -\sum_{\substack{ s,y\in \mathcal{D} \\ h = \operatorname{enc}(s) } } \sum_{i=1}^\docSize \omega(y_i) \log p\left(y_i|y_{<i},
h \right)\]
over the training data $\mathcal{D}$
using stochastic gradient descent with the ADAM optimizer
\cite{kingma2014adam}.
$\omega(0)=1$ and $\omega(1) = N_0/N_1$ where $N_y$ is the number of 
training examples with label $y$.
    We trained for a maximum of 50 epochs and the best
    model was selected with early stopping on the validation set according
    to ROUGE-2. {\color{red}{Each epoch constitutes a full pass through the
    dataset. The average stopping epoch was: CNN-DailyMail, 16.2; NYT, 21.36; DUC, 37.11; Reddit, 36.59; AMI, 19.58; PubMed, 19.84.
    }} All experiments were repeated with five random
    %\hal{there's prolly a bunch here that could go to the appendix}
    initializations.     Unless specified, word embeddings were initialized 
    using pretrained GloVe embeddings \cite{pennington2014glove} and we did 
    not update them during training. Unknown words were mapped to a zero 
    embedding.
    See \autoref{app:optset} for more optimization and training details.

{\color{red}
\subsection{Baselines}
\paragraph{Lead} As a baseline we include the lead summary, i.e. taking the first 
$x$ words of the document as summary, where $x$ is the target summary length for each dataset (see the 
first paragraph of \S~\ref{sec:exps}). While incredibly simple, this method is still a 
competitive baseline for single document summarization, especially on newswire.
\paragraph{Oracle} To measure the performance ceiling,
we show the ROUGE/METEOR scores using the 
extractive summary which results from greedily optimizing ROUGE-1. I.e., if we 
had clairvoyant knowledge
of the human reference summary, the oracle system achieves the (approximate) 
maximum possible ROUGE scores. 
See \autoref{app:oracle} for a detailed
description of the oracle algorithm.
}
%We choose ROUGE-2
%recall as our main evaluation metric since it has the strongest correlation
%to human content selection decisions.

%\kathy{Save all results for results section. I deleted your sentence.}
%\hal{i agree with kathy. put all the results together. be specific about what questions you're asking and then how you framed them as an experiment and then what the answer is. i think i'd just remove all this stuff here.}
%In most cases, the averaging encoder performance was as good or better than
%the RNN and CNN encoders, we use only the averaging encoder for the remainder
%of the experiments.

%\paragraph{Word Embedding Learning}{To futher understand how word 
%embeddings 
%can
%effect model performance we also compared extractors when embeddings 
%are updated during training. Both fixed and learned embedding variants are 
%initialized with GloVe embeddings. When learning embeddings, words occurring 
%three or fewer times in the training data are mapped to a learned unkown
%token.}


%We are also interested in the effect of lead bias. It is well known that the first few sentences of a news article, often referred to as the lead, make a good summary, and this is most commonly used as the default baseline in single document summarization. This lead bias is such a strong learning signal that the learned models almost always extract sentences from the lead despite the ground truth labeling containing a significant portion of positive labels later in the document. This begs the question, are we learning a robust model of sentence salience or simply identifying linguistic style features that are indicative of the lead?

%To better understand this phenomenon, 






%%% Local Variables:
%%% mode: latex
%%% TeX-master: "dlextsum.emnlp18"
%%% End:
