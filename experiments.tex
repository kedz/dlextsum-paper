\kathy{Could Lapata or Nallapati criticize your choice of parameters here as not being the same as what they used? (Note that I don't know what they used but it seems like a possible criticism.)}
We train all models to minimize the weighted negative log likelihood
\[\mathcal{L} = -\sum_{s,y\in \mathcal{D}} \sum_{i=1}^\docSize \omega(y_i) \log p\left(y_i|y_{<i},
\operatorname{enc}(\sent)\right)\]
over the training data $\mathcal{D}$
using stochastic gradient descent with the ADAM optimizer
\hal{maybe just say $\omega(0)=1$ and $\omega(1) = N_0/N_1$ where $N_y$ is the number of examples with label $y$.}
\cite{kingma2014adam}. The term 
\[ \omega(y_i)=\begin{cases}
\frac{\sum_{y_j} \mathbbm{1}\{y_j = 0 \}}{ \sum_{y_j} \mathbbm{1}\{y_j = 1 \}  } & \textrm{if $y_i=1$} \\ 1 & \textrm{otherwise}  \end{cases} \] 
        upweights
    positive examples proportionally to the ratio of negative to positive
    examples in the training data in order to compensate for the sparsity
    of the positive class.
\kathy{Could you say how you arrived at these settings? Is any one system penalized by the use of uniform settings?}
    We use a learning rate of .0001 and a dropout rate of .25 for all dropout
    layers. We also employ gradient clipping ($-5 < \nabla_\theta < 5$).
    We train for a maximum of 30 epochs and the best
    model is selected with early stopping on the validation set according
    to ROUGE-2. All experiments are repeated with five random\hal{there's prolly a bunch here that could go to the appendix}
    initializations. Weight matrix parameters are initialized using 
    Xavier initialization with the normal distribution 
    \cite{glorot2010understanding} and bias terms are set to 0.
    Unless specified, word embeddings are initialized 
    using pretrained GloVe embeddings \cite{pennington2014glove} and we do 
    not update them during training. Unknown words are mapped to a zero 
    embedding.
    We use a batch size of 32 for all datasets except AMI and PubMed for
    which we use sizes two and four respectively. 
\kathy{why? Say.}
    For the Cheng \& Lapata model, we train for half of the maximum epochs 
    with teacher forcing, i.e. we set $p_i = 1$
    if $y_i = 1$ in the gold data and 0 otherwise 
    when computing the decoder input 
    $p_i \cdot \sentEmb[i]$; we revert to the true model probability 
    during the second half training.

\subsection{Ground Truth Extract Summaries}
    \hal{i feel like this should go before the optimizer dtails}
Since we do not typically have ground truth extract summaries from which to
create the labels $\slabel_i$, we construct gold label sequences 
by greedily optimizing ROUGE-1. Starting with an empty summary $\summary = 
\null \emptyset$, we add the sentence $\hat{\sent} = 
{\argmax}_{ \sent \in \{\sent_1, \ldots, \sent_{\docsize}\},
\; \sent \notin \summary} \operatorname{ROUGE-1}(\summary \cup \sent)$
to $\summary$ stopping when the ROUGE-1 score no longer increases or the 
length budget is reached\hal{what does this mean. does it mean that if the $\hat s$ that's selected puts you over length limit then you stop, or is it that you stop when there are no more sentences that you can cram in? and what if adding $\hat s$ doesn't make rouge go up?}. We choose to optimize for ROUGE-1 rather than 
ROUGE-2 similarly to other optimization based approaches to summarization 
\textcolor{red}{
\cite{durrett2016learning,sipos2012large,nallapati2017summarunner} which found this
be the easier optimization target.}


\hal{do you say anywhere how you do significance testing and what significance level you use?}




\paragraph{Extractor/Encoder Comparions}{
In our main experiment, we compare our proposed 
sentence extractors, RNN and Seq2Seq,
to those of Cheng \& Lapata and SummaRunner.
We test all possible sentence extractor/encoder pairs across all the datasets
described in Section~\ref{sec:datasets}.} 
%We choose ROUGE-2
%recall as our main evaluation metric since it has the strongest correlation
%to human content selection decisions.

\kathy{Save all results for results section. I deleted your sentence.}
\hal{i agree with kathy. put all the results together. be specific about what questions you're asking and then how you framed them as an experiment and then what the answer is. i think i'd just remove all this stuff here.}
%In most cases, the averaging encoder performance was as good or better than
%the RNN and CNN encoders, we use only the averaging encoder for the remainder
%of the experiments.

\paragraph{Word Embedding Learning}{To futher understand how word 
embeddings 
can
effect model performance we also compared extractors when embeddings 
are updated during training. Both fixed and learned embedding variants are 
initialized with GloVe embeddings. When learning embeddings, words occurring 
three or fewer times in the training data are mapped to a learned unkown
token.}

\paragraph{POS Tag Ablation}{Additionally, we ran ablation experiments
using part-of-speech (POS) tags. \hal{this needs to be justified. why is this experiment interesting?}
All datasets were automatically tagged using
the SpaCy POS tagger \footnote{https://github.com/explosion/spaCy}.   
\kathy{I'm still curious what would happen if you separately removed all conjunction tags and later remaining POS.}
We experimented with selectively removing 
\begin{itemize}
    \item nouns (NOUN and PROPN tags), 
    \item verbs (VERB, PART, and AUX tags), 
    \item adjectives/adverbs (ADJ and ADV tags), 
    \item numerical expressions (NUM and SYM tags), and 
    \item miscellaneous words (ADP, CONJ, CCONJ, DET, INTJ, and SCONJ tags)
\end{itemize}
from each sentece. 
The embeddings of removed words were replaced with a zero vector,
preserving the order and position of the non-ablated words in the sentence.
}

\textbf{Document Shuffling} In examining the outputs of the models, we found
most of the selected sentences in the news domain came from the lead paragraph\hal{i feel like there must be citations to dig up here from like the 90s about lead summarization in news... it's also an intentional bias: maybe the right thing is to cite a style guide from a newsppaer that says to write this way}
of the document. This is despite the fact that there is a long tail of 
sentence extractions from later in the document in the ground truth extract 
summaries\hal{can you be more specific? like give some stats? what \%age come from first quarter of doc and what \%age from last half or something}. Because this lead bias is so strong, it is questionable whether
the models are learning to identify important content or just find the start
of the document. We perform a series of sentence order experiments where 
each document's sentences are randomly shuffled during training. We then
%KM - I think below should be shuffled. I changed.
%CK - models are trained on shuffled data but evaluated on in order models.
%evaluate each model performance on the unshuffled test data, comparing to 
evaluate each model performance on the unshuffled test data, comparing to 
the model trained on unshuffled data. 

%We are also interested in the effect of lead bias. It is well known that the first few sentences of a news article, often referred to as the lead, make a good summary, and this is most commonly used as the default baseline in single document summarization. This lead bias is such a strong learning signal that the learned models almost always extract sentences from the lead despite the ground truth labeling containing a significant portion of positive labels later in the document. This begs the question, are we learning a robust model of sentence salience or simply identifying linguistic style features that are indicative of the lead?

%To better understand this phenomenon, 




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "dlextsum.emnlp18"
%%% End:
