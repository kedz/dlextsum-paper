
We evaluate summary quality using ROUGE-2 recall \cite{lin2004rouge} 
as our main evaluation metric;
ROUGE-1 and ROUGE-LCS trend similarity to ROUGE-2 in our experiments.
We use target word lengths of 100 words for the news domains, and 
75, 290, and 200 words for Reddit, AMI, and PubMed respectively.
We also evaluate using METEOR \cite{denkowski:lavie:meteor-wmt:2014},
which measures precision and recall of reference words, allowing for
more complicated word matchings (e.g. via synonymy or morphology).



%\kathy{Could Lapata or Nallapati criticize your choice of parameters here as not being the same as what they used? (Note that I don't know what they used but it seems like a possible criticism.)}
We train all models to minimize the weighted negative log-likelihood
\[\mathcal{L} = -\sum_{s,y\in \mathcal{D}} \sum_{i=1}^\docSize \omega(y_i) \log p\left(y_i|y_{<i},
\operatorname{enc}(\sent)\right)\]
over the training data $\mathcal{D}$
using stochastic gradient descent with the ADAM optimizer
\cite{kingma2014adam}.
$\omega(0)=1$ and $\omega(1) = N_0/N_1$ where $N_y$ is the number of 
training examples with label $y$.
%    \kathy{Could you say how you arrived at these settings? Is any one system penalized by the use of uniform settings?}
\hal{what about kathy's question?}
    We train for a maximum of 30 epochs and the best
    model is selected with early stopping on the validation set according
    to ROUGE-2. All experiments are repeated with five random
    %\hal{there's prolly a bunch here that could go to the appendix}
    initializations.     Unless specified, word embeddings are initialized 
    using pretrained GloVe embeddings \cite{pennington2014glove} and we do 
    not update them during training. Unknown words are mapped to a zero 
    embedding.
    See \autoref{app:optset} for more optimization and training details.

%\paragraph{Extractor/Encoder Comparions}{\hal{why is this here rather than in results? maybe call this section ``experimental design'' or ``experimental settings'' or something like that, and then shift all results-like-things to results.}
%In our main experiment, we compare our proposed 
%sentence extractors, RNN and Seq2Seq,
%to those of Cheng \& Lapata and SummaRunner.
%We test all possible sentence extractor/encoder pairs across all the datasets
%described in Section~\ref{sec:datasets}.} 
As baselines we include a random summary baseline, i.e. randomly selecting sentences until word length $x$ is reached; a lead baseline, using 
the first $x$ words;  and a tail baseline, using the last $x$ words
($x$ is the target summary length for each domain).
To measure the approximate performance ceiling,
we also show the oracle performance using the 
summary which results from greedily optimizing ROUGE-1 

%We choose ROUGE-2
%recall as our main evaluation metric since it has the strongest correlation
%to human content selection decisions.

%\kathy{Save all results for results section. I deleted your sentence.}
%\hal{i agree with kathy. put all the results together. be specific about what questions you're asking and then how you framed them as an experiment and then what the answer is. i think i'd just remove all this stuff here.}
%In most cases, the averaging encoder performance was as good or better than
%the RNN and CNN encoders, we use only the averaging encoder for the remainder
%of the experiments.

%\paragraph{Word Embedding Learning}{To futher understand how word 
%embeddings 
%can
%effect model performance we also compared extractors when embeddings 
%are updated during training. Both fixed and learned embedding variants are 
%initialized with GloVe embeddings. When learning embeddings, words occurring 
%three or fewer times in the training data are mapped to a learned unkown
%token.}


%We are also interested in the effect of lead bias. It is well known that the first few sentences of a news article, often referred to as the lead, make a good summary, and this is most commonly used as the default baseline in single document summarization. This lead bias is such a strong learning signal that the learned models almost always extract sentences from the lead despite the ground truth labeling containing a significant portion of positive labels later in the document. This begs the question, are we learning a robust model of sentence salience or simply identifying linguistic style features that are indicative of the lead?

%To better understand this phenomenon, 




To perform statistical significance, we first average each document level score
(either ROUGE-2 or METEOR) over the five random initializations to get
one score per document per system per dataset.
We then test the difference between the best system on each dataset and 
all other systems using the approximate randomization test 
\cite{riezler2005some} with the Bonferroni correction for multiple comparisons,
testing for significance at the $\alpha=.05$ level. 




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "dlextsum.emnlp18"
%%% End:
