We experiment with three architectures for mapping sequences
of word embeddings to a fixed length vector: averaging, RNNs, and CNNs.
Hyperparameter settings and implementation details can be found 
in \autoref{app:sentencoders}.

\paragraph{Averaging Encoder} Under the averaging encoder, a sentence 
embedding \sentEmb is simply the average of its word embeddings, i.e. 
$\sentEmb = \frac{1}{\sentSize} \sum_{i=1}^{\sentSize} \wordEmb[i]$.

\paragraph{RNN Encoder} When using  the \textit{RNN} sentence encoder,
a sentence embedding is the concatenation of the final output states of a 
forward and backward RNN over the sentence's word embeddings. We use a Gated 
Recurrent Unit (GRU) for the RNN cell \cite{chung2014empirical}.

\paragraph{CNN Encoder} The \textit{CNN} sentence encoder uses a series of 
convolutional feature maps to encode each sentence. This encoder is similar
to the convolutional architecture of \citet{kim2014convolutional} used for 
text classification tasks and performs a series of ``one-dimensional'' 
convolutions over word embeddings. The final sentence embedding $\sentEmb$ is 
a concatenation of all the convolutional filter outputs after max pooling over
time.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "dlextsum.emnlp18"
%%% End:
