
%\hal{i feel like you can restructure the intro to focus on you stuff, rather than focusing on how it relates to other people's stuff. just lead with context selection is important for generation, summarization, etc., and is a key compontent both in extractive *and* abstractive techniques. then go to the third paragraph about what you do. you can then relate back to deep learning predictions in the related work section.}

%While there has a been a recent flurry of work on abstractive summarization
%\cite{paulus,see,chenglapata,nallapati},
%these papers treat this problem as a pure sequence to sequence 
%transduction task. Admittedly, this view allows us to apply very powerful, 
%general-purpose deep learning archictures to generate summaries.
%At the same time, it obscures a principal subtask in summarization, the 
%process of selecting the most salient units of meaning in the source material,
%i.e. the key ingredients in the final summary, a process which we 
%broadly refer to as content selection \cite{possiblyMcKeownAndNenkova}.

%As is also the case in other NLP tasks, it is not immediately obvious how a
%deep learning model is making its predictions, or what correlations 
%are being exploited. There is a concerning and growing list of papers that 
%find models functioning as mere nearest neighbors search 
%\cite{liang,danqichen}, exploiting annotator artifacts 
%\cite{recentNaaclPapersOnSNLI}, or open to adversarial exploitation \cite{findExampleYouNoMemoryDumbDumb}. 
%These lines of research are critical for finding model shortcomings, and over
%time, guiding improvements in technique. Unfortunately,
%to the best of our knowledge, there has been
%no such undertaking for the summarization task. 


%\kathy{content selection is very different for generation than for summarization.
%I think you need to initially say that content selection for generation is sentence
%selection. Note that you could be criticized even here as some people may say that
%summarization should be phrase selection (although it rarely is)}
%\kathy{I have also reworded definition of content selection for NLG.}
Content selection is an important sub-task in many natural language generation
%problems, 
tasks,
where, given a generation goal, the system must determine which information
%i.e. given some context, determine which information
needs to be expressed in output text \cite{gatt2018survey}.
In summarization, content selection usually is accomplished through sentence (and,
less frequently, phrase) extraction.
 While it is a key component of both
extractive and abstractive summarization systems, it is not generally well 
understood, \hal{well understood as a task or in specific models?} with frequency and information theoretic measures used as proxies
for content salience \cite{hong2014improving}. 
%changed wording

In this paper, we seek to better understand how deep learning models of 
%summarization are performing content selection across a variety of domains.
summarization perform content selection across a variety of domains.
%no new paragraph needed here
We perform an analysis 
of several recent sentence extractive neural network architectures, 
specifically considering the design choices for sentence encoders and 
extractors. We compare these architectures against
a simpler approach based on averaging of word embeddings in order to understand
the gains derived from more sophisticated architectures.
%
%\kathy{I would find it better to introduce approach here. I've added an
%extra sentence above. Because your experiments reveal the tidbits below by 
%contrasting past approaches with your simple approach which uses embedding
%averages}
%
%
%\kathy{You use some strong words, one of which is ``worrying''. I think it
%may be better toned down. After all, reviewers may be Lapata or Nallapati}
%\kathy{Also, I think it should be pumchier with all learned things itemized.
%For example:
%~
%KM2: Minor changes below
Our main results reveal:
\begin{enumerate}
\item Sentence position bias dominates the learning signal for news summarization, though not for
other genres\hal{be consistent on domain/genre. also maybe you should say above what domains you consider?}. Summary content for news is only slightly degraded when content words
are removed. 
\item Word embedding averaging is as good or better than either Recurrent Neural Net (RNN) or Convolutional Neural Net (CNN) encoders.\hal{across all domains?}
\item Pre-trained word embeddings are as good, or better than, learned embeddings in most cases.
\item Representation of previously selected summary content does not improve overall summary content. \hal{i think this one might be hard to parse for non-experts. maybe ``explicitly reprsenting prev...''}
\end{enumerate} 
%}
%\kathy{Then I would cut next three paragraphs but leaven in the one starting ``Taken together''}

%Our experiments reveal several worrying obstacles for learning such models. 
%For instance, the sentence position bias dominates the learning signal in the 
%news domain; this can be corrected for by shuffling a document's sentence
%order but not without adverse effects on performance.
%
%In our study of sentence encoder architectures, we find that word embedding
%averaging is as good or better than more sophisticated recurrent neural 
%network (RNN) and convolutional neural network (CNN) encoders.
%Additionally fixed pre-trained word embeddings are as good or better than 
%learned embeddings in the majority of cases. 
%
%We also propose two simple extractor methods that make sentence selection
%decisions independently in contrast to two previously published architectures
%where previous selection decisions inform future sentence selection probabilities. 
%Oddly we find that the independent predictions are as good as sequential 
%predictions.


%KM - Merged this paragraph with the next one.
Taken together, these and other results in the paper suggest that we are 
overestimating the ability of deep learning models to learn robust and 
meaningful features for summarization. 
%?impact of sentence position bias, the necessity
%?of learning embeddings, the unreasonable effectiveness of averaging for 
%?sentence embedding, and the cross domain generalizability of such models.
%?Additionally, we propose two simpler models that are on average statistically
%?indistinguishable from their more complex counterparts.

In this paper, we concentrate on sentence extraction approaches to summarization,
but believe that our findings are relevant to the abstractive summarization approaches as well.
The objective criteria for abstractive and extractive summarization and the the same:
produce output text with high word overlap to a reference human abstract. 
Moreover, most neural network-based abstractive summarization techniques are architecturally quite similar to
the extractive models we consider.

%\kathy{I think the contributions have to be the results of the study and the actual extractors. I've reworded.You may want to add these contributions once the domain adaptation part done. }
\kathy{I'm not sure these contributions are needed. Others should weigh in here.}
\hal{agreed. i think the first is already covered in the list above, and the second can probably be worked in to the second paragraph.}
The contributions of this paper are the following:
\begin{enumerate}
    \item An empirical study of extractive content selection in 
        deep learning 
        algorithms for text summarization across news, personal narratives, 
         meetings, and
         medical journal domains revealing difficulties in learning
         useful sentence representations.

    \item Two simple sentence extractor models whose performance is 
          on par 
          with more complex prior work.
\end{enumerate}

%\kathy{This seems detailed and slightly awkward. Are section numbers needed?}
%In the following sections we discuss 
%(Sec. ?) related work, 
%(Sec. ?) define the problem of extractive summarization, 
%(sec. ?) formulate our proposed ad baseline models,
%(Sec. ?) describe the datasets used for experiments,
%(Sec. ?) describe the experiments themselves, and conclude with 
%results and analysis (Sec. ?).


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "dlextsum.emnlp18"
%%% End:
